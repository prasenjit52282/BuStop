{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import itertools\n",
    "from library.correlationAndTtestLib import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_zone_cal(s):\n",
    "    '''\n",
    "    @param s: a string of format \"hh:mm:ss\"\n",
    "    \n",
    "    @return: a string denoting the timezone\n",
    "    '''\n",
    "    #print(s)\n",
    "    try:\n",
    "        hour=int(s.split(':')[0])\n",
    "    except:\n",
    "        hour=13 #for nan set to afternoon\n",
    "\n",
    "    if 6<hour<=9:\n",
    "        time_zone='Early_Morning'\n",
    "    elif 9<hour<=12:\n",
    "        time_zone='Morning'\n",
    "    elif 12<hour<=17:\n",
    "        time_zone='Afternoon'\n",
    "    elif 17<hour<=23:\n",
    "        time_zone='Evening'\n",
    "    return time_zone\n",
    "\n",
    "# converts hour:minute:seconds to seconds\n",
    "def time_as_int(timestr):\n",
    "    '''\n",
    "    @param timestr: a string of format \"hh:mm:ss\"\n",
    "    \n",
    "    @return: an integer denoting the time in seconds\n",
    "    '''\n",
    "    hh, mm, ss = timestr.split(\":\")\n",
    "    time_int = (int(hh)*3600)+(int(mm)*60)+int(ss)\n",
    "    return time_int\n",
    "\n",
    "def get_day_of_week(date_string, seperator=\"/\", date_format=\"mm/dd/yyyy\"):\n",
    "    '''\n",
    "    @param date_string: a string denoting a date\n",
    "    @param seperator (optional): a character which acts as seperator in the date_string. Default = \"/\"\n",
    "    @param date_format (optional): a string which denotes the format the date string is. Default = \"mm/dd/yyyy\"\n",
    "                                Supported values are : {\"dd/mm/yyyy\", \"mm/dd/yyyy\"}\n",
    "    \n",
    "    @return: a string denoting the day of week corressponding to the date denoted by date_string\n",
    "    '''\n",
    "    date_format_idxs = {'mm/dd/yyyy':{'date_idx': 1, 'month_idx': 0, 'year_idx': 2},\n",
    "                        'dd/mm/yyyy':{'date_idx': 0, 'month_idx': 1, 'year_idx': 2}}\n",
    "    try:\n",
    "        idx_dict = date_format_idxs[date_format]\n",
    "        date_arr = [int(val) for val in date_string.split(seperator)]\n",
    "        weekdays = {0:\"Monday\", 1:\"Tuesday\", 2:\"Wednesday\", 3:\"Thursday\", 4:\"Friday\", 5:\"Saturday\", 6:\"Sunday\"}\n",
    "        date = datetime.datetime(date_arr[idx_dict['year_idx']],\\\n",
    "                                 date_arr[idx_dict['month_idx']],\\\n",
    "                                 date_arr[idx_dict['date_idx']])\n",
    "        return weekdays[date.weekday()]\n",
    "    except KeyError:\n",
    "        raise ValueError(f\"{date_format} is not supported. Supported values are: {list(date_format_idxs.keys())}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "zones = [\"junct_mall\", \"prantika_bus_stand\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = r\"../reports/54ft/\"\n",
    "instance_wise_scores_dir = \"instance_wise_scores\"\n",
    "\n",
    "dates = [\"14_09_2019\", \"15_09_2019\", \"16_09_2019\", \"17_09_2019\", \"20_09_2019\", \"21_09_2019\", \"23_09_2019\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "zone_timezone_analysis_dfs = []\n",
    "for zone in zones:\n",
    "    path = os.path.join(log_dir, zone, f\"time_zone_analysis/score_timezone_analysis_{zone}_zone.csv\")\n",
    "    zone_timezone_analysis_dfs.append(pd.read_csv(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Welch's (t_test_stat, p_value) for zones ['junct_mall', 'prantika_bus_stand']: (6.9628689398734664, 2.0936879865449384e-05)\n"
     ]
    }
   ],
   "source": [
    "array_1, array_2 = zone_timezone_analysis_dfs[0]['Whole_Day'], zone_timezone_analysis_dfs[1]['Whole_Day']\n",
    "zonal_welch_t_result = welch_t_test(array_1, array_2)\n",
    "print(f\"Welch's (t_test_stat, p_value) for zones {zones}: {zonal_welch_t_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_dfs = {}\n",
    "\n",
    "for zone in zones:\n",
    "    for date in dates:\n",
    "        score_dfs[zone] = score_dfs.get(zone, {})\n",
    "        score_dfs[zone][date] = pd.read_csv(os.path.join(log_dir, zone, \n",
    "                                                      instance_wise_scores_dir, \n",
    "                                                      f\"{date}.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# test\n",
    "for zone in score_dfs.keys():\n",
    "    for date in dates:\n",
    "        temp_df = score_dfs.get(zone).get(date)\n",
    "        temp_df['Date'] = [date for i in range(len(temp_df))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "53\n48\n"
     ]
    }
   ],
   "source": [
    "joined_dfs = {}\n",
    "for zone in score_dfs.keys():\n",
    "    temp_df = pd.concat(score_dfs[zone].values(), ignore_index=True)\n",
    "    joined_dfs[zone] = temp_df\n",
    "    print(len(temp_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for temp_df in joined_dfs.values():\n",
    "    temp_df['Timezone'] = temp_df['start_time'].apply(lambda x : time_zone_cal(x))\n",
    "    temp_df['Day'] = temp_df['Date'].apply(lambda x : get_day_of_week(x, seperator=\"_\", date_format=\"dd/mm/yyyy\"))\n",
    "#     print(temp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "For junct_mall: \n\tFor 14_09_2019:\n\t\tAfternoon, Morning: 2,3\n\t\tAfternoon, Morning:(1.0, 0.42264973081037427)\n\n\tFor 15_09_2019:\n\t\tOnly 1 timezone available:\t[('15_09_2019', 'Afternoon')].\n\n\tFor 16_09_2019:\n\t\tAfternoon, Evening: 7,2\n\t\tAfternoon, Evening:(0.6868028197434451, 0.6029039678713222)\n\n\tFor 17_09_2019:\n\t\tAfternoon, Evening: 3,2\n\t\tAfternoon, Evening:(-1.8410785575880018, 0.1660703195925413)\n\n\tFor 20_09_2019:\n\t\tAfternoon, Evening: 6,1\n\t\tAfternoon, Evening:(nan, nan)\n\n\tFor 21_09_2019:\n\t\tAfternoon, Evening: 4,1\n\t\tAfternoon, Evening:(nan, nan)\n\n\tFor 23_09_2019:\n\t\tAfternoon, Evening: 4,1\n\t\tAfternoon, Evening:(nan, nan)\n\nFor prantika_bus_stand: \n\tFor 14_09_2019:\n\t\tOnly 1 timezone available:\t[('14_09_2019', 'Afternoon')].\n\n\tFor 15_09_2019:\n\t\tOnly 1 timezone available:\t[('15_09_2019', 'Afternoon')].\n\n\tFor 16_09_2019:\n\t\tOnly 1 timezone available:\t[('16_09_2019', 'Afternoon')].\n\n\tFor 17_09_2019:\n\t\tOnly 1 timezone available:\t[('17_09_2019', 'Afternoon')].\n\n\tFor 20_09_2019:\n\t\tOnly 1 timezone available:\t[('20_09_2019', 'Afternoon')].\n\n\tFor 21_09_2019:\n\t\tOnly 1 timezone available:\t[('21_09_2019', 'Afternoon')].\n\n\tFor 23_09_2019:\n\t\tOnly 1 timezone available:\t[('23_09_2019', 'Afternoon')].\n\n"
     ]
    }
   ],
   "source": [
    "for zone in joined_dfs.keys():\n",
    "    print(f\"For {zone}: \")\n",
    "    grouped = joined_dfs[zone].groupby([\"Date\", \"Timezone\"])\n",
    "    for date in dates:\n",
    "        print(f\"\\tFor {date}:\")\n",
    "        welch_test_pair = []\n",
    "        for pair in grouped.groups.keys():\n",
    "            if date in pair:\n",
    "                welch_test_pair.append(pair)\n",
    "                if len(welch_test_pair) == 2:\n",
    "                    break\n",
    "        if not len(welch_test_pair) < 2:\n",
    "            pair_1, pair_2 = welch_test_pair\n",
    "            array_1 = list(grouped.get_group(pair_1)['score'])\n",
    "            array_2 = list(grouped.get_group(pair_2)['score'])\n",
    "            \n",
    "            print(f\"\\t\\t{pair_1[1]}, {pair_2[1]}: {len(array_1)},{len(array_2)}\")\n",
    "            print(f\"\\t\\t{pair_1[1]}, {pair_2[1]}:\"\\\n",
    "                +f\"{welch_t_test(array_1, array_2)}\")\n",
    "        else:\n",
    "            print(f\"\\t\\tOnly {len(welch_test_pair)} timezone available:\\t{welch_test_pair}.\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "For Afternoon:\n\n\tFor 14_09_2019:\n\t\tjunct_mall, prantika_bus_stand: 2,2\n\t\t\t(1.6666666666666667, 0.3440417392452613)\n\tFor 15_09_2019:\n\t\tjunct_mall, prantika_bus_stand: 5,2\n\t\t\t(3.9999999999999996, 0.01613008990009254)\n\tFor 16_09_2019:\n\t\tjunct_mall, prantika_bus_stand: 7,11\n\t\t\t(3.9262435111551293, 0.002270496890740338)\n\tFor 17_09_2019:\n\t\tjunct_mall, prantika_bus_stand: 3,11\n\t\t\t(0.7513176183148906, 0.5092833464614165)\n\tFor 20_09_2019:\n\t\tjunct_mall, prantika_bus_stand: 6,4\n\t\t\t(1.0744680507152484, 0.315703633645757)\n\tFor 21_09_2019:\n\t\tjunct_mall, prantika_bus_stand: 4,9\n\t\t\t(4.899086389260638, 0.0011951492036522944)\n\tFor 23_09_2019:\n\t\tjunct_mall, prantika_bus_stand: 4,9\n\t\t\t(2.3206768503158806, 0.09260893211811398)\n\n"
     ]
    }
   ],
   "source": [
    "a, b = joined_dfs.keys()\n",
    "a_group = joined_dfs[a].groupby([\"Date\", \"Timezone\"])\n",
    "b_group = joined_dfs[b].groupby([\"Date\", \"Timezone\"])\n",
    "for timezone in set(joined_dfs[zone]['Timezone']):\n",
    "    print(f\"For {timezone}:\\n\")\n",
    "    for date in dates:\n",
    "        print(f\"\\tFor {date}:\")\n",
    "        pair = (date, timezone)\n",
    "        a_exists = pair in a_group.groups.keys()\n",
    "        b_exists = pair in b_group.groups.keys()\n",
    "        if(a_exists or b_exists):\n",
    "            if(a_exists and b_exists):\n",
    "                a_array = list(a_group.get_group(pair)['score'])\n",
    "                b_array = list(b_group.get_group(pair)['score'])\n",
    "                print(f\"\\t\\t{a}, {b}: {len(a_array)},{len(b_array)}\")\n",
    "                print(f\"\\t\\t\\t{welch_t_test(a_array, b_array)}\")\n",
    "            else:\n",
    "                print(f\"\\t\\tFor pair: {pair}, only available: \" + a if a_exists else b)\n",
    "        else:\n",
    "            print(\"\\t\\tNothing found!\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}